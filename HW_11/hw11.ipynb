{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Домашнее задание к лекции \"Основы веб-скрапинга\"**\n",
    "\n",
    "**Обязательная часть**\n",
    "\n",
    "Вам необходимо написать функцию, которая будет основана на поиске по сайту habr.com. Функция в качестве параметра должна принимать список запросов для поиска (например, ['python', 'анализ данных']) и на основе материалов, попавших в результаты поиска по каждому запросу, возвращать датафрейм вида:\n",
    "\n",
    "<дата> - <заголовок> - <ссылка на материал>\n",
    "\n",
    "В рамках задания предполагается работа только с одной (первой) страницей результатов поисковой выдачи для каждого запроса. Материалы в датафрейме не должны дублироваться, если они попадали в результаты поиска для нескольких запросов из списка.\n",
    "\n",
    "**Дополнительная часть (необязательная)**\n",
    "\n",
    "Функция из обязательной части задания должна быть расширена следующим образом:\n",
    "\n",
    "кроме списка ключевых слов для поиска необходимо объявить параметр с количеством страниц поисковой выдачи. Т.е. при передаче в функцию аргумента 4 необходимо получить материалы с первых 4 страниц результатов;\n",
    "в датафрейме должны быть столбцы с полным текстом найденных материалов и количеством лайков:\n",
    "<дата> - <заголовок> - <ссылка на материал> - <текст материала> - <количество лайков>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Необходимые импорты"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import re\n",
    "\n",
    "pd.options.display.max_colwidth = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Список ключевых слов для запросов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keyword_list = ['python', 'анализ данных', 'deepseek', 'пенза'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_queries(query_list: list):\n",
    "\n",
    "    \"\"\"Функция для формирования датафрейма(ов) на основании переданного\n",
    "    списка ключевых слов. \n",
    "\n",
    "    Returns:\n",
    "        list: список датафреймов, сформированных на основании элементов переданного\n",
    "            в качестве параметра списка.\n",
    "    \"\"\"\n",
    "\n",
    "    url = 'https://habr.com/ru/search/'\n",
    "    query_result_list = []\n",
    "\n",
    "    # пройдемся по переданному списку ключевых слов\n",
    "    for query in query_list:\n",
    "        rows = []\n",
    "        # передадим в параметры запроса ключевое слово\n",
    "        params = {\n",
    "            'q': query,\n",
    "            'target_types': 'posts',\n",
    "            'order': 'date'\n",
    "        }\n",
    "\n",
    "        # выполним запрос и создадим объект BeautifulSoup на основании полученного\n",
    "        # ответа для дальнейшей обработки\n",
    "        res = requests.get(url, params)\n",
    "        time.sleep(0.2)\n",
    "        soup = BeautifulSoup(res.text)\n",
    "\n",
    "        # заберем все ссылки на первой странице выдачи в список и сделаем их \n",
    "        # полными\n",
    "        refs_list = soup.find_all('a', class_='tm-title__link')\n",
    "\n",
    "        links_list = []\n",
    "        all_links = []\n",
    "\n",
    "        for ref in refs_list:\n",
    "            links_list.append(ref.get('href'))\n",
    "\n",
    "        full_link = list(map(lambda x: 'https://habr.com' + x, links_list))\n",
    "\n",
    "        all_links += full_link\n",
    "\n",
    "        # используя полученный список ссылок, сделаем по ним запросы, чтобы\n",
    "        # извлечь требуемые данные - дату, заголовок и саму ссылку\n",
    "        for link in all_links:\n",
    "            res = requests.get(link)\n",
    "            time.sleep(0.2)\n",
    "            soup = BeautifulSoup(res.text)\n",
    "            res_datetime = soup.find(\n",
    "                'span', class_='tm-article-datetime-published').find('time')['datetime']\n",
    "            \n",
    "            res_datetime = datetime.fromisoformat(res_datetime.replace('Z', '+00:00'))\n",
    "            res_datetime = res_datetime.strftime(\"%d.%m.%Y\")\n",
    "\n",
    "            title = soup.find('h1', class_='tm-title tm-title_h1').find('span').text\n",
    "            row = {'date':res_datetime,\n",
    "                   'title': title,\n",
    "                   'link': link}\n",
    "            rows.append(row)\n",
    "\n",
    "        # на основании полученного списка сформируем датафрейм и добавим его\n",
    "        # в возвращаемый результат работы функции\n",
    "        query_df = pd.DataFrame(rows).reset_index(drop=True)\n",
    "        query_result_list.append(query_df)\n",
    "\n",
    "    # поработаем над удалением дублей новостей, встречающихся в разных запросах\n",
    "\n",
    "    # Колонка, по которой проверяем дубликаты\n",
    "    column_to_check = 'link'\n",
    "\n",
    "    # Собираем все значения из всех датафреймов\n",
    "    all_values = pd.concat([df[column_to_check] for df in query_result_list])\n",
    "\n",
    "    # Находим дубликаты\n",
    "    duplicates = all_values[all_values.duplicated(keep=False)].unique()\n",
    "\n",
    "    # Удаляем дубликаты из всех датафреймов, кроме первого\n",
    "    for i, df in enumerate(query_result_list):\n",
    "        if i > 0:  # Пропускаем первый датафрейм\n",
    "            query_result_list[i] = df[~df[column_to_check].isin(duplicates)]\n",
    "\n",
    "    return query_result_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for df in get_queries(keyword_list):\n",
    "    display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_queries_advanced(query_list :list, num_pages: int = 1):\n",
    "    \"\"\"\n",
    "    get_queries_advanced Функция для формирования датафрейма(ов) на основании переданного\n",
    "    списка ключевых слов и количества нужных страниц выдачи\n",
    "\n",
    "    Arguments:\n",
    "        query_list -- список ключевых слов [str]\n",
    "\n",
    "    Keyword Arguments:\n",
    "        num_pages -- количество страниц выдачи (default: {1})\n",
    "\n",
    "    Returns:\n",
    "        список датафреймов\n",
    "    \"\"\"\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    query_result_list = []\n",
    "\n",
    "    # пройдемся по переданному списку ключевых слов\n",
    "    for query in query_list:\n",
    "        # передадим в параметры запроса ключевое слово\n",
    "        params = {\n",
    "                'q': query,\n",
    "                'target_types': 'posts',\n",
    "                'order': 'date'\n",
    "            }\n",
    "        \n",
    "        rows = []\n",
    "\n",
    "        # получим необходимое количество страниц выдачи и пройдемся по ним\n",
    "        for num in range(1, num_pages+1):\n",
    "            url = f'https://habr.com/ru/search/page{num}/'\n",
    "                        \n",
    "            # выполним запрос и создадим объект BeautifulSoup на основании полученного\n",
    "            # ответа для дальнейшей обработки\n",
    "            res = requests.get(url, params)\n",
    "            time.sleep(0.2)\n",
    "            soup = BeautifulSoup(res.text)\n",
    "\n",
    "            # заберем все ссылки на странице выдачи в список и сделаем их \n",
    "            # полными\n",
    "            refs_list = soup.find_all('a', class_='tm-title__link')\n",
    "\n",
    "            links_list = []\n",
    "            all_links = []\n",
    "\n",
    "            for ref in refs_list:\n",
    "                links_list.append(ref.get('href'))\n",
    "\n",
    "            full_link = list(map(lambda x: 'https://habr.com' + x, links_list))\n",
    "\n",
    "            all_links += full_link\n",
    "\n",
    "            # используя полученный список ссылок, сделаем по ним запросы, чтобы\n",
    "            # извлечь требуемые данные\n",
    "            for link in all_links:\n",
    "                res = requests.get(link)\n",
    "                if res.status_code == 200:\n",
    "                    time.sleep(0.2)\n",
    "                    soup = BeautifulSoup(res.text)\n",
    "                    res_datetime = soup.find('span', class_='tm-article-datetime-published').find('time')['datetime']\n",
    "                    \n",
    "                    res_datetime = datetime.fromisoformat(res_datetime.replace('Z', '+00:00'))\n",
    "                    res_datetime = res_datetime.strftime(\"%d.%m.%Y\")\n",
    "\n",
    "                    title = soup.find('h1', class_='tm-title tm-title_h1').find('span').text\n",
    "\n",
    "                    article_text = soup.find('div', class_='tm-article-body').get_text(separator='\\n')\n",
    "\n",
    "                    search_options = [\n",
    "                        {'tag': 'div', 'class_': 'tm-votes-lever tm-votes-lever tm-votes-lever_appearance-article tm-article-rating__votes-switcher'},\n",
    "                        {'tag': 'div', 'class_': 'tm-votes-meter__value tm-votes-meter__value_positive tm-votes-meter__value_appearance-article tm-votes-meter__value_rating tm-votes-meter__value'},\n",
    "                        {'tag': 'span', 'class_': 'tm-votes-meter__value'},\n",
    "                    ]\n",
    "\n",
    "                    for option in search_options:\n",
    "                        reactions = soup.find(option['tag'], class_=option['class_'])  \n",
    "                        if reactions is not None:  \n",
    "                            reactions = soup.find(option['tag'], class_=option['class_']).get('title')\n",
    "                            break\n",
    "                    if reactions:    \n",
    "                        pattern = r\"\\u2191(\\d+)\"  # Ищем число после стрелки вверх\n",
    "                        positive_reactions = re.search(pattern, reactions).group(1)\n",
    "                    else:\n",
    "                        positive_reactions = 0\n",
    "\n",
    "                    row = {'date':res_datetime,\n",
    "                        'title': title,\n",
    "                        'link': link,\n",
    "                        'article_text': article_text,\n",
    "                        'likes': positive_reactions}\n",
    "                    rows.append(row)\n",
    "            \n",
    "        # на основании полученного списка сформируем датафрейм и добавим его\n",
    "        # в возвращаемый результат работы функции\n",
    "        query_df = pd.DataFrame(rows).reset_index(drop=True)\n",
    "        query_result_list.append(query_df)\n",
    "\n",
    "    # поработаем над удалением дублей новостей, встречающихся в разных запросах\n",
    "\n",
    "    # Колонка, по которой проверяем дубликаты\n",
    "    column_to_check = 'link'\n",
    "\n",
    "    # Собираем все значения из всех датафреймов\n",
    "    all_values = pd.concat([df[column_to_check] for df in query_result_list])\n",
    "\n",
    "    # Находим дубликаты\n",
    "    duplicates = all_values[all_values.duplicated(keep=False)].unique()\n",
    "\n",
    "    # Удаляем дубликаты из всех датафреймов, кроме первого\n",
    "    for i, df in enumerate(query_result_list):\n",
    "        if i > 0:  # Пропускаем первый датафрейм\n",
    "            query_result_list[i] = df[~df[column_to_check].isin(duplicates)]\n",
    "\n",
    "    return query_result_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_list = get_queries_advanced(keyword_list, 2)\n",
    "\n",
    "for df in df_list:\n",
    "    display(df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
